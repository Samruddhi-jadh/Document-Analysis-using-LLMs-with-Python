{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Document Analysis using LLMs with Python**"
      ],
      "metadata": {
        "id": "9X082_3ufv8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj_duNKffoDF",
        "outputId": "c05d153d-9fa0-4787-f1b9-8b4637063b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.9)\n",
            "Requirement already satisfied: pdfminer.six==20251230 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20251230)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (5.3.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "mviRC3cmuHY8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "pdf_path = \"/content/google_cloud_ai_agent_trends_2026_report.pdf\"\n",
        "output_text_file= \"text_extract.txt\"\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "  extracted_text = \"\"\n",
        "  for page in pdf.pages:\n",
        "    extracted_text+=page.extract_text()\n",
        "with open(output_text_file, \"w\") as text_file:\n",
        "  text_file.write(extracted_text)\n",
        "print(f\"Text extracted and saved to {output_text_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80kbDhyxgCz8",
        "outputId": "3e8e1fd1-11cf-4208-e959-a3652f93f9a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted and saved to text_extract.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the extracted content by taking preview\n",
        "with open(output_text_file, \"r\") as file:\n",
        "  document_text=file.read()\n",
        "print(document_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dLM7vodhfWX",
        "outputId": "26a7b5ac-ecd6-4014-8874-e9b37cee8abd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI agent\n",
            "trends\n",
            "2026\n",
            "Five shifts that will\n",
            "redefine roles, workflows,\n",
            "and business value in 2026.\n",
            "1About this\n",
            "report\n",
            "This report provides key insights for business\n",
            "leaders to shape their AI agent strategy for\n",
            "2026 and beyond. Within each trend, you will\n",
            "find real-life examples, technical resources,\n",
            "and customer stories to share with your\n",
            "teams for deeper learning.\n",
            "These trends were identified using a blend\n",
            "of qualitative and quantitative data, including\n",
            "internal Google Cloud and Google DeepMind\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Summarize the document with pretrained summarization model named t5-small from transformers\n",
        "from transformers import pipeline\n",
        "#load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "summary = summarizer(document_text, max_length=150, min_length=30, do_sample=False) #do_sample=False to get deterministic output\n",
        "print(\"Summary:\", summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpqEz74PiYZG",
        "outputId": "adc26594-4520-46ed-ed18-31d61bd6fdc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7345 > 512). Running this sequence through the model will result in indexing errors\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: AI is transforming the security landscape in both offense and defense . it is a fundamental shift in the role of the employee as strategic . the key to achieving this gap is to build a faster, smarter workforce . if you are a business owner, you should be able to work with a specialized agent .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the document into sentences and passages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(document_text)  # List of sentences\n",
        "\n",
        "passages = []\n",
        "current_passage = \"\"\n",
        "\n",
        "for sentence in sentences:\n",
        "    # word count check\n",
        "    if len(current_passage.split()) + len(sentence.split()) < 20:\n",
        "        current_passage += \" \" + sentence\n",
        "    else:\n",
        "        passages.append(current_passage.strip())\n",
        "        current_passage = sentence\n",
        "\n",
        "# add last passage\n",
        "if current_passage:\n",
        "    passages.append(current_passage.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuczNweYlMz5",
        "outputId": "7825e2b5-7f8a-44f1-c9bb-5a28956e76eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the Cluster → Select → Generate strategy to reduce computational cost\n",
        "1. Split the document into passages\n",
        "2. Select the most informative passages based on heuristic importance\n",
        "3. Generate questions only from the selected passages using an LLM\n"
      ],
      "metadata": {
        "id": "9MlK9TrSwkUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deduplicate_questions(questions):\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for q in questions:\n",
        "        nq = q.lower().strip()\n",
        "        if nq not in seen:\n",
        "            seen.add(nq)\n",
        "            unique.append(q)\n",
        "    return unique\n"
      ],
      "metadata": {
        "id": "sOqWUvOt-8KD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicate Question Handling:\n",
        "1. Since LLM-based question generation often produces repeated or near-duplicate questions, a normalization-based deduplication strategy is applied.\n",
        "2. Questions are lowercased and stripped of whitespace, and only unique questions are retained after each generation step to improve diversity and reduce redundancy.\n"
      ],
      "metadata": {
        "id": "0Br3GRr0DOo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Questions From The Passage Using LLms\n",
        "qg_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"valhalla/t5-base-qg-hl\"\n",
        ")\n",
        "\n",
        "def generate_questions(passage, min_questions=5):\n",
        "    results = qg_pipeline(f\"generate diverse questions: {passage}\")\n",
        "\n",
        "    questions = results[0][\"generated_text\"].split(\"<sep>\")\n",
        "    questions = [q.strip() for q in questions if q.strip()]\n",
        "    questions = deduplicate_questions(questions)\n",
        "\n",
        "    if len(questions) < min_questions:\n",
        "        sentences = passage.split(\". \")\n",
        "        for i in range(len(sentences)):\n",
        "            if len(questions) >= min_questions:\n",
        "                break\n",
        "\n",
        "            extra = \" \".join(sentences[i:i+2])\n",
        "            r = qg_pipeline(f\"generate questions: {extra}\")\n",
        "            more_qs = r[0][\"generated_text\"].split(\"<sep>\")\n",
        "            questions.extend([q.strip() for q in more_qs if q.strip()])\n",
        "            questions = deduplicate_questions(questions)\n",
        "\n",
        "    return questions[:min_questions]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4TFJKzin24X",
        "outputId": "9ed9456f-48ed-4953-c961-c2abde1ec3b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented a hybrid QA system where extractive models are used for factual questions and a context-grounded LLM is invoked only for reasoning-based questions using a retrieval-augmented setup. This significantly reduced hallucinations and optimized compute cost."
      ],
      "metadata": {
        "id": "gBKFoj7cbEA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_text(text, max_words=300):\n",
        "    return \" \".join(text.split()[:max_words])\n",
        "\n",
        "\n",
        "def clean_context(text):\n",
        "    text = re.sub(r\"\\b\\d{4}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def clean_repetition(text):\n",
        "    words = text.split()\n",
        "    cleaned = []\n",
        "    for w in words:\n",
        "        if len(cleaned) < 3 or cleaned[-3:] != [w] * 3:\n",
        "            cleaned.append(w)\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "\n",
        "def clean_extractive_answer(ans):\n",
        "    ans = re.sub(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+.*?(Google|Forbes|Cloud).*\", \"\", ans)\n",
        "    ans = re.sub(r\"\\d+\", \"\", ans)\n",
        "    ans = ans.strip(\"”\\\"- ,.\")\n",
        "    return ans\n",
        "\n",
        "\n",
        "def normalize_sentence(text):\n",
        "    sents = sent_tokenize(text)\n",
        "    if not sents:\n",
        "        return \"\"\n",
        "    sent = sents[0]\n",
        "    if not sent.endswith(\".\"):\n",
        "        sent += \".\"\n",
        "    return sent\n",
        "\n",
        "\n",
        "def is_trivial_answer(ans):\n",
        "    if len(ans.split()) < 6:\n",
        "        return True\n",
        "    if sum(c.isdigit() for c in ans) / max(len(ans), 1) > 0.2:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def confidence_score(mode, extractive_score=None):\n",
        "    if mode == \"extractive\":\n",
        "        return round(extractive_score, 2)\n",
        "    if mode == \"abstractive\":\n",
        "        return 0.75\n",
        "    if mode == \"extractive-fallback\":\n",
        "        return 0.60\n",
        "    return 0.0"
      ],
      "metadata": {
        "id": "KbdinvPSt1jY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QA Models\n",
        "extractive_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"deepset/roberta-base-squad2\"\n",
        ")\n",
        "\n",
        "abstractive_llm = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    device=0\n",
        ")\n",
        "\n",
        "ABSTRACTIVE_HINTS = [\n",
        "    \"why\", \"how\", \"role\", \"impact\", \"benefits\",\n",
        "    \"future\", \"expected\", \"significance\",\n",
        "    \"types\", \"how are\", \"what could\"\n",
        "]\n",
        "\n",
        "def needs_abstraction(question):\n",
        "    q = question.lower()\n",
        "    return any(h in q for h in ABSTRACTIVE_HINTS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe9P3VZRud2g",
        "outputId": "67db1b70-57b1-4ea7-c6ab-59039e322b7e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Context Builder\n",
        "def build_context(passage, all_passages, max_words=300):\n",
        "    context = [passage]\n",
        "    for p in all_passages:\n",
        "        if p != passage:\n",
        "            context.append(p)\n",
        "        if sum(len(c.split()) for c in context) >= max_words:\n",
        "            break\n",
        "\n",
        "    return truncate_text(clean_context(\" \".join(context)), max_words)\n",
        "\n",
        "\n",
        "def abstractive_answer(question, context):\n",
        "    prompt = f\"\"\"\n",
        "Answer the question using ONLY the context.\n",
        "Write a complete, clear sentence.\n",
        "Avoid names, citations, numbers, and lists.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    output = abstractive_llm(\n",
        "        prompt,\n",
        "        max_new_tokens=120,\n",
        "        repetition_penalty=1.3,\n",
        "        do_sample=False\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    output = clean_repetition(output.strip())\n",
        "    return normalize_sentence(output)\n"
      ],
      "metadata": {
        "id": "WurGYyz_urO0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, passage, all_passages):\n",
        "    passage = truncate_text(passage)\n",
        "\n",
        "    #  Extractive\n",
        "    ext = extractive_qa(question=question, context=passage)\n",
        "    cleaned_ext = clean_extractive_answer(ext[\"answer\"])\n",
        "\n",
        "    if ext[\"score\"] >= 0.35 and not is_trivial_answer(cleaned_ext):\n",
        "        final = normalize_sentence(cleaned_ext)\n",
        "        return final, \"extractive\", confidence_score(\"extractive\", ext[\"score\"])\n",
        "\n",
        "    #  Abstractive\n",
        "    context = build_context(passage, all_passages)\n",
        "    abs_ans = abstractive_answer(question, context)\n",
        "\n",
        "    if not is_trivial_answer(abs_ans):\n",
        "        return abs_ans, \"abstractive\", confidence_score(\"abstractive\")\n",
        "\n",
        "    #  Safe fallback (still meaningful)\n",
        "    fallback = normalize_sentence(truncate_text(passage, 35))\n",
        "    return fallback, \"extractive-fallback\", confidence_score(\"extractive-fallback\")"
      ],
      "metadata": {
        "id": "Aca9aSm4u4zO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "important_passages = sorted(\n",
        "    passages,\n",
        "    key=lambda x: len(x.split()),\n",
        "    reverse=True\n",
        ")[:10]\n",
        "\n",
        "seen_questions = set()\n",
        "\n",
        "for idx, passage in enumerate(important_passages):\n",
        "    print(f\"\\n================ Passage {idx+1} ================\\n\")\n",
        "\n",
        "    questions = generate_questions(passage)\n",
        "\n",
        "    questions = [q for q in questions if q.lower() not in seen_questions]\n",
        "    for q in questions:\n",
        "        seen_questions.add(q.lower())\n",
        "\n",
        "    if not questions:\n",
        "        continue\n",
        "\n",
        "    print(\"Generated Questions:\")\n",
        "    for q in questions:\n",
        "        print(\"-\", q)\n",
        "\n",
        "    print(\"\\nAnswers:\\n\")\n",
        "\n",
        "    for q in questions:\n",
        "        ans, mode, conf = answer_question(q, passage, important_passages)\n",
        "\n",
        "        print(f\"Q: {q}\")\n",
        "        print(f\"A: {ans}\")\n",
        "        print(f\"Mode: {mode}\")\n",
        "        print(f\"Confidence: {conf}\")\n",
        "        print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Vk1SQluiKi",
        "outputId": "01b3f47d-472f-469d-9011-572a7f3b902c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ Passage 1 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What is the focus of the Cloud Learning Services Market Pulse?\n",
            "- What is the name of the market Pulse that is fielded Sept.-Nov 2024?\n",
            "- What is the name of the country in the U.S. in 2024?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What is the focus of the Cloud Learning Services Market Pulse?\n",
            "A: Cloud Learning Services Market Pulse will be the year when every employee can go from guessing to knowing—but only if their organizations invest in the skills to make it possible.” Andrew Milo Global Director, Customer Training, Cloud Learning Services, Google Cloud 7 Forbes, AI Puts The Squeeze On The Shrinking Half-life Of Skills, 391 2 3 4 5 Agents for scale What executives are saying:8 of decision-makers agree that 82% technical learning resources help their organization stay ahead in AI of organizations surveyed realize 71% an increase in revenue since engaging.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "Q: What is the name of the market Pulse that is fielded Sept.-Nov 2024?\n",
            "A: Google/Ipsos, Cloud Learning Services Market Pulse.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "Q: What is the name of the country in the U.S. in 2024?\n",
            "A: U.S., U.K., FR, DE, IN, BR, MX, JP, AU/N.Z.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 2 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What is the most significant business shift of 2026?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What is the most significant business shift of 2026?\n",
            "A: It’s a fundamental change in workflow, a new way to work that will require a profound shift in mindset and corporate culture.”.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 3 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What is one of the benefits of using the Expanded Secure AI Framework 2.0?\n",
            "- What is the role of AI in democratizing healthcare?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What is one of the benefits of using the Expanded Secure AI Framework 2.0?\n",
            "A: they only need to be right once, address the rapidly emerging risks posed by while the defender has to be right every autonomous AI agents.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "Q: What is the role of AI in democratizing healthcare?\n",
            "A: This would allow preemptive risk management across patient populations and the democratization of high-quality healthcare.” Aashima Gupta Director, Healthcare, Global Strategic Industries, Google Cloud 301 2 3 Trend 4 5 Agents for security Advancing security from aler ts to action 311 2 3 4 Agents for security 5 In a modern security operations center (SOC), human analysts face a constant stream of data and alerts, with 82% concerned or very concerned that they may be missing real threats or incidents due to the amount of alert.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 4 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What is the role of agents in the media and entertainment industry?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What is the role of agents in the media and entertainment industry?\n",
            "A: help in the understanding of vast amounts of complex content and data.\n",
            "Mode: extractive\n",
            "Confidence: 0.44\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 5 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What types of agents are involved in the agentic SOC?\n",
            "- What does the agentic SOC cycle through after receiving a security alert?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What types of agents are involved in the agentic SOC?\n",
            "A: Escalation Human managed Recommendation Human managed Alert Detection AI agents Triage and Threat research investigation and hunt Malware analysis Detection Response engineering Support for Model Context Protocol (MCP) to build custom agentic workflows across your entire security stack, including third-party tools This dynamic process of evaluating, acting, Multiple SOC agents need common enterprise and re-evaluating enables the system to context and can share the same security data adapt to a changing security environment sources (e.g., security telemetry data), regularly in.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "Q: What does the agentic SOC cycle through after receiving a security alert?\n",
            "A: A process, engaging various agents: Escalation Human managed Recommendation Human managed Alert Detection AI agents Triage and Threat research investigation and hunt Malware analysis Detection Response engineering Support for Model Context Protocol (MCP) to build custom agentic workflows across your entire security stack, including third-party tools This dynamic process of evaluating, acting, Multiple SOC agents need common enterprise and re-evaluating enables the system to context and can share the same security data adapt to a changing security environment sources (e.g., security.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 6 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What could agents do for every workflow?\n",
            "- What are the benefits of using agents for every workflow?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What could agents do for every workflow?\n",
            "A: 2 Agents for every workflow 3 4 5 Digital assembly line: Orchestrating agentic systems A digital assembly line While LLMs are the “brains” of these agents, they have two major limitations: their knowledge is made possible by the is frozen at the time of their training, and they Agent2Agent (A2A) protocol.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "Q: What are the benefits of using agents for every workflow?\n",
            "A: Agents could autonomously remediate network anomalies, proactively open a ticket with the field service systems, and alert contact centers to inform customers of a technician dispatch, all in one integrated sequence.” Angelo Libertucci Director, Telecom, Global Strategic Industries, Google Cloud 171 2 Agents for every workflow 3 4 5 Digital assembly line: Orchestrating agentic systems A digital assembly line While LLMs are the “brains” of these agents, they have two major limitations: their knowledge is made possible by the is frozen at the time of their training,.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 7 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What is one of the most important aspects of a security professional's job?\n",
            "- What is one of the most critical elements of a security professional's job?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What is one of the most important aspects of a security professional's job?\n",
            "A: Security professionals must be deeply bilingual in both AI and security, to stay ahead of advanced AI threats from bad actors, and defend against them with sophisticated AI tools.” Francis deSouza COO and President, Security Products, Google Cloud 371 2 3 4 Trend 5 Agents for scale Upskilling talent will be the ultimate driver of business value 381 2 3 4 5 Agents for scale It is tempting to focus on the technology—the models, the platforms, and prompts—but this misses the most critical element: the people.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "Q: What is one of the most critical elements of a security professional's job?\n",
            "A: security professionals must be deeply bilingual in both AI and security, to stay ahead of advanced AI threats from bad actors, and defend against them with sophisticated AI tools.” Francis deSouza COO and President, Security Products, Google Cloud 371 2 3 4 Trend 5 Agents for scale Upskilling talent will be the ultimate driver of business value 381 2 3 4 5 Agents for scale It is tempting to focus on the technology—the models, the platforms, and prompts—but this misses the most critical element: the people.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 8 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What type of systems can monitor regulatory changes, identify impacted policies, update internal workflows, and create a complete audit chain?\n",
            "- What is one way to make your customer service automation more efficient?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What type of systems can monitor regulatory changes, identify impacted policies, update internal workflows, and create a complete audit chain?\n",
            "A: For example, we’ll see multi-step agentic compliance systems that can monitor regulatory changes, identify impacted policies, update internal workflows, and create a complete audit chain, making the process more efficient.” Toby Brown Managing Director, Financial.\n",
            "Mode: extractive-fallback\n",
            "Confidence: 0.6\n",
            "--------------------------------------------------\n",
            "Q: What is one way to make your customer service automation more efficient?\n",
            "A: Multi-step agentic compliance systems that can monitor regulatory changes, identify impacted policies, update internal workflows, and create a complete audit chain.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 9 ================\n",
            "\n",
            "Generated Questions:\n",
            "- How are AI agents leveraged across the enterprise?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: How are AI agents leveraged across the enterprise?\n",
            "A: will be the year when every employee can go from guessing to knowing—but only if their organizations invest in the skills to make it possible.” Andrew Milo Global Director, Customer Training, Cloud Learning Services, Google Cloud 7 Forbes, AI Puts The Squeeze On The Shrinking Half-life Of Skills, 391 2 3 4 5 Agents for scale What executives are saying:8 of decision-makers agree that 82% technical learning resources help their organization stay ahead in AI of organizations surveyed realize 71% an increase in revenue since engaging with learning resources What employees are saying.\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n",
            "\n",
            "================ Passage 10 ================\n",
            "\n",
            "Generated Questions:\n",
            "- What is the expected return on investment of gen AI?\n",
            "- What is the expected ROI of gen AI?\n",
            "\n",
            "Answers:\n",
            "\n",
            "Q: What is the expected return on investment of gen AI?\n",
            "A: AI agents will transform complex, multi-step processes like procurement, security operations and customer support— shifting the human roles to focus on high-value, strategic orchestration across the business.” Francis deSouza COO and President, Security Products, Google.\n",
            "Mode: extractive-fallback\n",
            "Confidence: 0.6\n",
            "--------------------------------------------------\n",
            "Q: What is the expected ROI of gen AI?\n",
            "A: In what timeframe do you expect gen AI to deliver return on investment (ROI) to the following areas of your business?).\n",
            "Mode: abstractive\n",
            "Confidence: 0.75\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}